{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16-ロジスティック回帰.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTC-Y61bk9tU",
        "colab_type": "text"
      },
      "source": [
        "#16章 ロジスティック回帰\n",
        "\n",
        "　広く用いられている教師あり学習技術のひとつである。\n",
        "\n",
        "　\"回帰\"という名前だが、分類問題を扱う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTsUcWD850tV",
        "colab_type": "text"
      },
      "source": [
        "##レシピ16.1 2クラス分類器の訓練\n",
        "###問題\n",
        "　簡潔なクラス分類モデルを訓練したい。\n",
        "###解決策\n",
        "　`scikit-learn`の`LogisticRegression`を用いて、ロジスティック回帰器を訓練する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rCRghkpkt53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ライブラリをロード\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#データをロードし、特徴量を2つに制限\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data[:100,:]\n",
        "target = iris.target[:100]\n",
        "\n",
        "#特徴量を標準化\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "#ロジスティック回帰器を作成\n",
        "logistic_regression = LogisticRegression(random_state=0)\n",
        "\n",
        "#ロジスティック回帰器を訓練\n",
        "model = logistic_regression.fit(features_standardized, target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmSQoqJckNcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h67Ut8aFAsAj",
        "colab_type": "text"
      },
      "source": [
        "###議論\n",
        "　ロジスティック回帰は2クラス分類器(つまりターゲットベクトルが2つの値のうち1つを取る)で広く用いられている。ロジスティック回帰では線形モデル($\\beta_0$ + $\\beta_1x$など)がロジスティック関数(シグモイド関数とも呼ばれる)$\\frac{1}{1+e^{-(\\beta_0+\\beta_1x)}}$の中に組み込まれている。\n",
        "\n",
        "$$\n",
        "P(y_i = 1|X) = \\frac{1}{1+e^{-(\\beta_0+\\beta_1x)}}\n",
        "$$\n",
        "\n",
        " ここで、\n",
        "\n",
        "$P(y_i=1|X)$: $i$番目の観測値$y_i$がクラスに属する確率\n",
        "\n",
        "$X$: 訓練データ\n",
        "\n",
        "$\\beta_0, \\beta_1$: 訓練時に学習されるパラメータ\n",
        "\n",
        "ロジスティック関数の効果は、関数の出力値を0から1の間に制約し、確率として解釈できるようにすることである。\n",
        "$P(y_i-1|X)$が0.5よりも大きければ、クラス1であると予測される。そうでなければクラス0と予測される。\n",
        "\n",
        "`scikit-learn`では`LogisticRegression`としてロジスティック回帰が実装されている。モデルを訓練すれば、それを用いて観測値のクラスを予測できる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1IEtw9X6eB3",
        "colab_type": "code",
        "outputId": "5c4ab3d6-228f-4aed-b4f1-5a5e1bdfb666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#新たな観測値を作成\n",
        "new_observation = [[.5, .5, .5, .5]]\n",
        "\n",
        "#クラスを予測\n",
        "model.predict(new_observation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca21YD3qNnd4",
        "colab_type": "text"
      },
      "source": [
        "この例では、観測値は1と予測されている。\n",
        "\n",
        "さらに、観測値がそれぞれのクラスに属する確率を見ることもできる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCU80DyTNmFH",
        "colab_type": "code",
        "outputId": "3a9e157a-8067-4bca-98cc-191ddb2ade77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#予測確率を表示\n",
        "model.predict_proba(new_observation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.17738424, 0.82261576]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jtppdq_SMsf",
        "colab_type": "text"
      },
      "source": [
        "この観測値は、クラス0に属する確率が18.8%、**クラス1**に属する確率が81.1%となっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wj12WMZSg7O",
        "colab_type": "text"
      },
      "source": [
        "##レシピ16.2 多クラス分類器の訓練\n",
        "###問題\n",
        "　　3つ以上のクラスに対するクラス分類器を訓練したい。\n",
        "###解決策\n",
        "　　`scikit-learn`の`LogisticRegression`を使って、1対その他法(one-vs-rest)か、多項法(multinomial)を用いてロジスティック回帰器を訓練する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV0g5ichQRo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ライブラリをロード\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#データをロード\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "target = iris.target\n",
        "\n",
        "#特徴量を標準化\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "#1対その他法ロジスティック回帰器を作成\n",
        "logistic_regression = LogisticRegression(random_state=0, multi_class=\"ovr\")\n",
        "\n",
        "#ロジスティック回帰器を訓練\n",
        "model = logistic_regression.fit(features_standardized, target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3VUzv4HoHJL",
        "colab_type": "code",
        "outputId": "fbf6161c-5742-44bd-9e87-5c4fdfb08c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_observation = [[4.6, 2.7, 1.3, 0.1]]\n",
        "model.predict(new_observation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPCZNNZJo8OL",
        "colab_type": "code",
        "outputId": "f71ad4b6-3c51-490d-c1e9-ca5f63eb2b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict_proba(new_observation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00467529, 0.17485605, 0.82046866]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2wT3uUfVgx3",
        "colab_type": "text"
      },
      "source": [
        "###議論\n",
        "　ロジスティック回帰器そのものは2クラス分類器なので、3つ以上のクラスを持つターゲットベクトルを扱うことはできない。しかし、ロジスティック回帰には、多クラスを扱うための2種類の巧妙な拡張がなされている。\n",
        "\n",
        "　1つ目は、1対その他法ロジスティック回帰(OVR: one-vs-rest logistic regression)と呼ばれるもので、個々のクラスに対してそのクラスに属するかどうかだけを判断する(これは2クラス分類になる)独立したモデルを訓練する。この方法は、個々のクラス分類問題(クラス0かどうか、など)は互いに独立であることを仮定している。\n",
        "\n",
        "　もう一つの方法は、多項ロジスティック回帰(MLR: Multinomial logistic regression)と呼ばれるもので、「レシピ15.1 観測値の近傍の発見」で示した式のロジスティック関数を下記の式で置き換えたものである。\n",
        "$$\n",
        "P(y_i=k|X)=\\frac{e^{\\beta_kx_i}}{\\sum_{j=1}^{K}e^{\\beta_ix_i} \\quad}\n",
        "$$\n",
        "\n",
        "ここで、\n",
        "\n",
        "$P(y_i=k|X)$: $i$番目の観測値\n",
        "\n",
        "$y_i$: クラス$k$に属する確率\n",
        "\n",
        "$K$: クラスの総数\n",
        "\n",
        "多項ロジスティック回帰の実用上のメリットは、`predict_proba`で得られる予測確率の信頼性が高い(つまり、よく較正されている)ことである。\n",
        "\n",
        "※較正されている(calibrated)：予測確率の値が正答率と一致していることを指す。つまり、予測確率が70%の観測値に対する正答率が70%であるならば、較正されている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVqQoS74dsOP",
        "colab_type": "text"
      },
      "source": [
        "##レシピ16.3 正則化によるバリアンスの削減\n",
        "###問題\n",
        "　ロジスティック回帰モデルのバリアンスを削減したい。\n",
        "###解決策\n",
        "　正則化強度を示すハイパーパラメータCを調節する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTGo-dhYXrfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ライブラリをロード\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#データをロード\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "target = iris.target\n",
        "\n",
        "#特徴量を標準化\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "#ロジスティック回帰器を作成\n",
        "logistic_regression = LogisticRegressionCV(\n",
        "    penalty='l2', Cs=10, random_state=0, n_jobs=-1)\n",
        "\n",
        "#ロジスティック回帰器を訓練\n",
        "model = logistic_regression.fit(features_standardized, target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koj_AQ8u0xWl",
        "colab_type": "code",
        "outputId": "0c869579-e274-44cf-ba3c-8c54b1a57aae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict(new_observation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LetYt5sY0-jT",
        "colab_type": "code",
        "outputId": "bbeb7870-7295-4030-9935-3dee7a8514fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict_proba(new_observation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00467529, 0.17485605, 0.82046866]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVGzaKPPhDxN",
        "colab_type": "text"
      },
      "source": [
        "#議論\n",
        "　正則化とは、複雑なモデルにペナルティを与えることで、バリアンスを低減する手法である。具体的には、最小化の対象となるロス関数にペナルティ項を追加する。ペナルティ項としては、L1やL2がよく用いられる。L1ペナルティは下のようになる。\n",
        "$$\n",
        "\\alpha\\sum_{j=1}^{p}|\\hat{\\beta_j}| \\quad\n",
        "$$\n",
        "ここで\n",
        "\n",
        "$\\hat{\\beta_j}$: $p$個ある特徴量のうち$j$番目の特徴量に対応する学習されるパラメータ(重み)\n",
        "\n",
        "$\\alpha$: 正則化強度を示すハイパーパラメータ\n",
        "\n",
        "L2は以下のようになる。\n",
        "$$\n",
        "\\alpha\\sum_{j=1}^{p}\\hat{\\beta^2_j} \\quad\n",
        "$$\n",
        "$\\alpha$を大きくすると、パラメータの値が大きい(つまりモデルが複雑である)ことに対するペナルティが増大する。scikit-learnでは、一般的な手法に従って、$\\alpha$ではなく$C$を指定する。ここでは$C$は正則化強度$\\alpha$の逆数、すなわち$C=\\frac{1}{\\alpha}$となる。ロジスティック回帰を使いながらバリアンスを削減するには、$C$をハイパーパラメータとして扱い、最良のモデルを与える$C$の値を探索すればよい。scikit-learnでは、`LogisticRegressionCV`クラスを用いて効率的に$C$を最適化することができる。`LogisticRegressionCV`のパラメータ`Cs`には$C$の探索範囲を浮動小数点のリストとして与えることもできるし、候補値の個数を整数で指定することもできる。後者の場合には、-10,000から10,000の範囲から対数スケールで候補値が抽出される。\n",
        "\n",
        "　残念ながら`LogisticRegressionCV`は、ペナルティ項にまたがった探索はできない。つまり、最良のペナルティ項とパラメータを同時に探索することはできない。それを行うには、「12章 モデル選択」で述べた非効率的なモデル選択手法をを使う必要がある。"
      ]
    }
  ]
}